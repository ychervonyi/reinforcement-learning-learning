{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cliff_walking_chapter6.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOGCxsQCdTkuVgqBxvj34rd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ychervonyi/reinforcement-learning-learning/blob/main/cliff_walking_chapter6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZfzqlEvdyim1"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "np.random.seed(1231231)\n",
        "\n",
        "\n",
        "class Environment:\n",
        "    def __init__(self, grid):\n",
        "        self.parse_grid(grid)\n",
        "        self.all_actions = [(-1, 0), (1, 0), (0, -1), (0, 1)]\n",
        "        self.n_actions = len(self.all_actions)\n",
        "\n",
        "    def parse_grid(self, grid):\n",
        "        self._grid = np.array([list(s.lstrip()) for s in grid.split(\"\\n\")[1:-1]])\n",
        "        self._n_rows, self._n_cols = self._grid.shape\n",
        "        self.state_tuple = self._grid.shape\n",
        "        self._start_points = []\n",
        "        self._finish_points = []\n",
        "        for r in range(self._n_rows):\n",
        "            for c in range(self._n_cols):\n",
        "                if self._grid[r, c] == \"S\":\n",
        "                    self._start_points.append((r, c))\n",
        "                elif self._grid[r, c] == \"F\":\n",
        "                    self._finish_points.append((r, c))\n",
        "\n",
        "    def _is_valid_point(self, r, c):\n",
        "        return 0 <= r < self._n_rows and 0 <= c < self._n_cols\n",
        "\n",
        "    def get_start_point(self):\n",
        "        return self._start_points[np.random.randint(0, len(self._start_points))]\n",
        "\n",
        "    def get_possible_action_indices(self, r, c):\n",
        "        possible_actions = []\n",
        "        for i, a_i in enumerate(self.all_actions):\n",
        "            dr, dc = a_i\n",
        "            if self._is_valid_point(r + dr, c + dc):\n",
        "                possible_actions.append(i)\n",
        "        return possible_actions\n",
        "\n",
        "    def step(self, r, c, policy_fn):\n",
        "        action_i = policy_fn(r, c)\n",
        "        dr, dc = self.all_actions[action_i]\n",
        "        new_r, new_c = r + dr, c + dc\n",
        "        if not self._is_valid_point(new_r, new_c):\n",
        "            reward = -1\n",
        "            new_r, new_c = r, c\n",
        "        elif self._grid[new_r, new_c] == \"F\":\n",
        "            reward = 0\n",
        "        elif self._grid[new_r, new_c] == \"X\":\n",
        "            reward = -100\n",
        "            new_r, new_c = self.get_start_point()\n",
        "        else:\n",
        "            reward = -1\n",
        "        return action_i, reward, new_r, new_c\n",
        "\n",
        "    def plot_trajectory(self, traj):\n",
        "        grid = np.zeros(self._grid.shape)\n",
        "        for r, c in traj:\n",
        "            grid[r][c] += 1\n",
        "        print(grid)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H_2mr6LXJWPv"
      },
      "source": [
        "class Agent:\n",
        "    \"\"\"\n",
        "    Agent contains Q function, policy and C. Policy is determined\n",
        "    from Q function.\n",
        "    Agent also contains policies.\n",
        "    \"\"\"\n",
        "    def __init__(self, env, epsilon=0.1):\n",
        "        self.env = env\n",
        "        # Value function\n",
        "        self.Q = np.random.uniform(\n",
        "            low=-1,\n",
        "            high=1,\n",
        "            size=self.env.state_tuple + (self.env.n_actions,)\n",
        "        )\n",
        "        # self.Q = np.zeros(self.env.state_tuple + (self.env.n_actions,))\n",
        "        for r, c in self.env._finish_points:\n",
        "            self.Q[r, c] = np.zeros(self.env.n_actions)\n",
        "\n",
        "        # Policy\n",
        "        self.policy = np.argmax(self.Q, axis=2)\n",
        "        # self.policy = np.zeros((rows, cols))\n",
        "        # self.policy_init_value = -1\n",
        "        # self.policy = np.full(self.env.state_tuple, self.policy_init_value)\n",
        "\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "    def epsilon_greedy_policy(self, r, c):\n",
        "        \"\"\"\n",
        "        Get possible actions from the environment,\n",
        "        choose one random action out of the possible actions with\n",
        "        probability `epsilon` or return learned action if it is allowed.\n",
        "\n",
        "        Returns action index.\n",
        "        \"\"\"\n",
        "        if np.random.rand() > self.epsilon:\n",
        "            a_index = self.policy[r, c]\n",
        "        else:\n",
        "            a_index = np.random.randint(0, len(self.env.all_actions))\n",
        "        return a_index"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KKmCFnnxhtPY"
      },
      "source": [
        "class QLearningControl:\n",
        "    def __init__(self, env, agent):\n",
        "        self.gamma = 1\n",
        "        self.alpha = 0.1\n",
        "        self.env = env\n",
        "        self.agent = agent\n",
        "\n",
        "    def optimize(self, train_episodes):\n",
        "        total_steps = ep_count = 0\n",
        "        for ep in range(train_episodes):\n",
        "            if ep == 10 or ep % 1000 == 0 and total_steps != 0:\n",
        "                print(f\"Average trajectory length: {round(total_steps / ep_count, 2)}\")\n",
        "                total_steps = ep_count = 0\n",
        "                self.env.plot_trajectory(traj)\n",
        "            r, c = self.env.get_start_point()\n",
        "            steps = 0\n",
        "            traj = []\n",
        "            while True:\n",
        "                new_state = self.env.step(r, c, self.agent.epsilon_greedy_policy)\n",
        "                traj.append((r, c))\n",
        "                action_i, reward, new_r, new_c = new_state\n",
        "                if reward == 0:\n",
        "                    traj.append((new_r, new_c))\n",
        "                    break\n",
        "                self.agent.Q[r, c, action_i] += self.alpha * (reward + self.gamma * np.amax(self.agent.Q[new_r, new_c]) - self.agent.Q[r, c, action_i])\n",
        "                self.agent.policy[r, c] = np.argmax(self.agent.Q[r, c])\n",
        "                steps += 1\n",
        "                r, c = new_r, new_c\n",
        "            total_steps += steps\n",
        "            ep_count += 1"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OCxXaBRlOLQp",
        "outputId": "9d0e4c1d-2e93-4a05-c2e2-24dae2592acb"
      },
      "source": [
        "grid = \"\"\"\n",
        "OOOOOOOOOOOOOOOOOOOO\n",
        "OOOOOOOOOOOOOOOOOOOO\n",
        "OOOOOOOOOOOOOOOOOOOO\n",
        "OOOOOOOOOOOOOOOOOOOO\n",
        "SXXXXXXXXXXXXXXXXXXF\n",
        "\"\"\"\n",
        "env = Environment(grid)\n",
        "agent = Agent(env)\n",
        "td_control = OffPolicyTDControl(env, agent)\n",
        "td_control.optimize(20000)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Average trajectory length: 692.9\n",
            "[[12.  9.  5.  4.  5.  4.  5.  5.  5.  5.  7.  6.  7.  7.  5.  3.  0.  0.\n",
            "   0.  0.]\n",
            " [10.  9.  7.  5.  8.  7.  6.  5.  6.  5.  7.  6.  7.  8.  7.  5.  3.  1.\n",
            "   0.  0.]\n",
            " [ 8.  8.  9.  7.  8.  7.  7.  5.  7.  7.  8.  8.  7.  7.  6.  5.  3.  0.\n",
            "   0.  0.]\n",
            " [11.  7.  6.  5.  5.  4.  6.  6.  5.  5.  5.  5.  5.  5.  4.  4.  2.  1.\n",
            "   1.  2.]\n",
            " [ 9.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "   0.  1.]]\n",
            "Average trajectory length: 46.24\n",
            "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 2. 0. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
            "Average trajectory length: 29.51\n",
            "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [3. 2. 2. 2. 2. 2. 2. 3. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
            "Average trajectory length: 30.03\n",
            "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [1. 1. 1. 1. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
            "Average trajectory length: 28.54\n",
            "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [2. 2. 2. 2. 3. 3. 2. 2. 2. 2. 2. 2. 2. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
            "Average trajectory length: 29.9\n",
            "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 1. 1. 2. 3. 2. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [2. 2. 2. 1. 2. 2. 1. 1. 1. 2. 1. 1. 1. 1. 1. 2. 2. 1. 1. 2.]\n",
            " [3. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
            "Average trajectory length: 29.37\n",
            "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
            "Average trajectory length: 29.29\n",
            "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
            "Average trajectory length: 29.07\n",
            "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [3. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
            "Average trajectory length: 30.57\n",
            "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0.]\n",
            " [1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
            "Average trajectory length: 30.49\n",
            "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
            "Average trajectory length: 30.35\n",
            "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 1. 0. 0. 2. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [3. 2. 3. 3. 2. 2. 2. 1. 0. 2. 1. 3. 2. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [3. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
            "Average trajectory length: 30.18\n",
            "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [6. 6. 6. 6. 6. 4. 4. 4. 4. 3. 3. 2. 2. 2. 1. 1. 1. 1. 1. 1.]\n",
            " [6. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
            "Average trajectory length: 30.07\n",
            "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [2. 2. 2. 2. 2. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2.]\n",
            " [2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
            "Average trajectory length: 29.31\n",
            "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
            "Average trajectory length: 30.88\n",
            "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [2. 3. 2. 2. 2. 2. 2. 2. 2. 2. 2. 3. 4. 3. 2. 2. 2. 2. 1. 1.]\n",
            " [2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
            "Average trajectory length: 30.1\n",
            "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            " [2. 2. 2. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 1. 1. 1. 1.]\n",
            " [5. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
            "Average trajectory length: 30.15\n",
            "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [3. 2. 1. 1. 1. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
            "Average trajectory length: 30.28\n",
            "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [5. 4. 5. 4. 5. 4. 4. 3. 3. 2. 2. 2. 2. 2. 2. 2. 2. 2. 3. 1.]\n",
            " [4. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
            "Average trajectory length: 29.75\n",
            "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 2. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [7. 7. 6. 9. 7. 6. 6. 7. 5. 4. 4. 4. 4. 4. 3. 2. 2. 2. 1. 1.]\n",
            " [6. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3PTb8CDAfDoa"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}