{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "racetrack_monte_carlo_chapter5_v3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP1ih6TLIJKOa/+/uUIGFVx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ychervonyi/reinforcement-learning-learning/blob/main/racetrack_monte_carlo_chapter5_v3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gMRc-0RVDUxU"
      },
      "source": [
        "import numpy as np\n",
        "from collections import Counter\n",
        "from matplotlib import pyplot\n",
        "from tqdm import tqdm\n",
        "\n",
        "np.random.seed(1231231)\n",
        "\n",
        "\n",
        "class Point:\n",
        "    def __init__(self, rr, cc):\n",
        "        self.y = rr\n",
        "        self.x = cc\n",
        "\n",
        "\n",
        "def collinear(p1, p2, p3):\n",
        "    \"\"\"\n",
        "    Return true iff p1, p2, and p3 all lie on the same line.\n",
        "    \"\"\"\n",
        "    return (p2.x - p1.x) * (p3.y - p1.y) == (p3.x - p1.x) * (p2.y - p1.y)\n",
        "\n",
        "\n",
        "def within(a, b, c):\n",
        "    \"\"\"\n",
        "    Return true iff b is between a and c (inclusive).\n",
        "    \"\"\"\n",
        "    return a <= b <= c or c <= b <= a\n",
        "\n",
        "\n",
        "class Segment:\n",
        "    def __init__(self, p1, p2):\n",
        "        self.p1 = p1\n",
        "        self.p2 = p2\n",
        "        self.A = p2.y - p1.y\n",
        "        self.B = p1.x - p2.x\n",
        "        self.C = self.A * p1.x + self.B * p1.y\n",
        "\n",
        "    def print(self):\n",
        "        print(f\"x1: {self.p1.x}, y1: {self.p1.y}\")\n",
        "        print(f\"x2: {self.p2.x}, y2: {self.p2.y}\")\n",
        "\n",
        "    def p_is_on(self, p):\n",
        "        return (collinear(self.p1, self.p2, p)\n",
        "                and (within(self.p1.x, p.x, self.p2.x) if self.p1.x != self.p2.x else\n",
        "                     within(self.p1.y, p.y, self.p2.y)))\n",
        "\n",
        "\n",
        "def segments_intersect(s1, s2):\n",
        "    \"\"\"\n",
        "    Check if two segments intersect.\n",
        "    \"\"\"\n",
        "    A1, B1, C1 = s1.A, s1.B, s1.C\n",
        "    A2, B2, C2 = s2.A, s2.B, s2.C\n",
        "    det = A1 * B2 - A2 * B1\n",
        "    if det == 0:\n",
        "        return False\n",
        "    x = (B2 * C1 - B1 * C2) / det\n",
        "    y = (A1 * C2 - A2 * C1) / det\n",
        "\n",
        "    for s in (s1, s2):\n",
        "        x1, x2 = s.p1.x, s.p2.x\n",
        "        y1, y2 = s.p1.y, s.p2.y\n",
        "        if not (min(x1, x2) <= x <= max(x1, x2) and min(y1, y2) <= y <= max(y1, y2)):\n",
        "            return False\n",
        "    return True\n",
        "\n",
        "\n",
        "def compare_points(p1, p2):\n",
        "    \"\"\"\n",
        "    Check if two points are the same.\n",
        "    \"\"\"\n",
        "    return p1.x == p2.x and p1.y == p2.y\n",
        "\n",
        "\n",
        "def compare_segments(s1, s2):\n",
        "    \"\"\"\n",
        "    Check if two segments are the same.\n",
        "    \"\"\"\n",
        "    return compare_points(s1.p1, s2.p1) and compare_points(s1.p2, s2.p2)\n",
        "\n",
        "\n",
        "class Environment:\n",
        "    def __init__(self):\n",
        "        self.build()\n",
        "\n",
        "    def build(self):\n",
        "        \"\"\"\n",
        "        Build an environment.\n",
        "        \"\"\"\n",
        "        print(\"Building an environment...\")\n",
        "        # NOTE: this is an ugly way to define the track geometry\n",
        "        # Here we define the track as a polygon\n",
        "        rows = 30\n",
        "        cols = 15\n",
        "\n",
        "        self.max_v, self.min_v = 5, 0\n",
        "        self.rows, self.cols = rows, cols\n",
        "        self.state_tuple = (rows, cols, self.max_v, self.max_v)\n",
        "\n",
        "        self.start_segment = Segment(Point(0, 0), Point(0, 10))\n",
        "        print(\"Start line:\")\n",
        "        self.start_segment.print()\n",
        "        self.finish_segment = Segment(Point(10, 15), Point(30, 15))\n",
        "        print(\"Finish line:\")\n",
        "        self.finish_segment.print()\n",
        "\n",
        "        # Define start points\n",
        "        self.start_points = []\n",
        "        r = 0\n",
        "        for c in range(1, 10):\n",
        "            self.start_points.append((r, c))\n",
        "\n",
        "        # Boundaries\n",
        "        points = [\n",
        "            Point(0, 0),\n",
        "            Point(0, 10),\n",
        "            Point(10, 10),\n",
        "            Point(10, 15),\n",
        "            Point(30, 15),\n",
        "            Point(30, 0)\n",
        "        ]\n",
        "\n",
        "        # Bounds should not have start and finish lines\n",
        "        self.bounds = []\n",
        "        for i in range(1, len(points)):\n",
        "            s = Segment(points[i - 1], points[i])\n",
        "            if compare_segments(s, self.finish_segment) or compare_segments(s, self.start_segment):\n",
        "                continue\n",
        "            print(f\"Adding boundary...\")\n",
        "            s.print()\n",
        "            self.bounds.append(s)\n",
        "\n",
        "        self.bounds.append(Segment(points[-1], points[0]))\n",
        "        print(f\"Adding boundary...\")\n",
        "        self.bounds[-1].print()\n",
        "\n",
        "        self.build_all_actions()\n",
        "\n",
        "    def build_all_actions(self):\n",
        "        \"\"\"\n",
        "        Build all actions.\n",
        "        \"\"\"\n",
        "        dv = [-1, 0, 1]\n",
        "        n_dv = len(dv)\n",
        "        self.all_actions_map = {}\n",
        "        self.all_actions = []\n",
        "        for i, d_vr in enumerate(dv):\n",
        "            for j, d_vc in enumerate(dv):\n",
        "                self.all_actions.append((d_vr, d_vc))\n",
        "                self.all_actions_map[f\"{d_vr}_{d_vc}\"] = i * n_dv + j\n",
        "        self.n_actions = len(self.all_actions)\n",
        "\n",
        "    def is_good_velocity(self, vr_, vc_):\n",
        "        \"\"\"\n",
        "        Check if velocity is allowed.\n",
        "        \"\"\"\n",
        "        if (vr_, vc_) == (\n",
        "        self.min_v, self.min_v) or vr_ < self.min_v or vr_ >= self.max_v or vc_ < self.min_v or vc_ >= self.max_v:\n",
        "            return False\n",
        "        return True\n",
        "\n",
        "    def get_possible_action_indices(self, vr_, vc_):\n",
        "        \"\"\"\n",
        "        Get possible actions for current velocity `vr_, vc_`.\n",
        "        \"\"\"\n",
        "        # 9 actions for each state - increase, decrease or\n",
        "        # don't change velocity components\n",
        "\n",
        "        possible_actions_indicies = []\n",
        "        for d_vr, d_vc in self.all_actions:\n",
        "            if self.is_good_velocity(vr_ + d_vr, vc_ + d_vc):\n",
        "                possible_actions_indicies.append(self.all_actions_map[f\"{d_vr}_{d_vc}\"])\n",
        "        return possible_actions_indicies\n",
        "\n",
        "    def get_episode(self, policy_fn):\n",
        "        \"\"\"\n",
        "        Generate an episode.\n",
        "        \"\"\"\n",
        "        hits = 0\n",
        "        reward = -1\n",
        "        vr = vc = 0\n",
        "        r, c = self.start_points[np.random.randint(0, len(self.start_points))]\n",
        "        finish = False\n",
        "        history = []\n",
        "        while not finish:\n",
        "            d_vr, d_vc, prob = policy_fn(r, c, vr, vc)\n",
        "            new_vr, new_vc = vr + d_vr, vc + d_vc\n",
        "\n",
        "            assert not (new_vr == 0 and new_vc == 0 and (r, c) in self.start_points), f\"{new_vr} {new_vc} {r} {c}\"\n",
        "\n",
        "            # Check if the new path intersects finish line\n",
        "            # If yes we are done with this episode\n",
        "            new_path = Segment(Point(r, c), Point(r + new_vr, c + new_vc))\n",
        "            if segments_intersect(self.finish_segment, new_path):\n",
        "                history.append((r, c, vr, vc, d_vr, d_vc, 0, prob))\n",
        "                break\n",
        "\n",
        "            # Check if the path intersects boundaries\n",
        "            # If yes we move the car to the start line\n",
        "            hit = False\n",
        "            for bound in self.bounds:\n",
        "                if segments_intersect(bound, new_path):\n",
        "                    hit = True\n",
        "                    # TODO: Worse reward for hitting a boundary?\n",
        "                    history.append((r, c, vr, vc, d_vr, d_vc, -5, prob))\n",
        "                    hits += 1\n",
        "                    r, c = self.start_points[np.random.randint(0, len(self.start_points))]\n",
        "                    vr = vc = 0\n",
        "\n",
        "            # Update velocity and position if didn't hit a bound\n",
        "            if not hit:\n",
        "                history.append((r, c, vr, vc, d_vr, d_vc, -1, prob))\n",
        "                vr, vc = new_vr, new_vc\n",
        "                r += vr\n",
        "                c += vc\n",
        "        return history, hits\n",
        "\n",
        "    def plot_episode(self, history):\n",
        "        track = np.zeros((self.rows, self.cols))\n",
        "        pyplot.imshow(track)\n",
        "        for item in history:\n",
        "            # r, c, vr, vc, d_vr, d_vc, reward, p = item\n",
        "            pyplot.plot(item[1], item[0], '.r')\n",
        "        pyplot.show()\n",
        "\n",
        "    def print_episode(self, history):\n",
        "        \"\"\"\n",
        "        Follow `history` and print how many times\n",
        "        each cell was visited.\n",
        "        \"\"\"\n",
        "        track = np.zeros((self.rows, self.cols))\n",
        "        for item in history:\n",
        "            # r, c, vr, vc, d_vr, d_vc, reward, p = item\n",
        "            track[item[0], item[1]] += 1\n",
        "        return track.astype(\"int\")"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EwtNOBzhDY8V"
      },
      "source": [
        "class Agent:\n",
        "    \"\"\"\n",
        "    Agent contains Q function, policy and C. Policy is determined \n",
        "    from Q function.\n",
        "    Agent also contains policies.\n",
        "    \"\"\"\n",
        "    def __init__(self, env, epsilon=0.1):\n",
        "        self.env = env\n",
        "        # Value function\n",
        "        self.Q = np.random.uniform(\n",
        "            low=0,\n",
        "            high=self.env.n_actions,\n",
        "            size=self.env.state_tuple + (self.env.n_actions,)\n",
        "        )\n",
        "        # self.Q = np.zeros(self.env.state_tuple + (self.env.n_actions,))\n",
        "\n",
        "        # Cumulative sum of weights\n",
        "        self.C = np.zeros(self.env.state_tuple + (self.env.n_actions,))\n",
        "        # Policy\n",
        "        # self.policy = np.argmax(Q, axis=4)\n",
        "        # self.policy = np.zeros((rows, cols))\n",
        "        self.policy_init_value = -1\n",
        "        self.policy = np.full(self.env.state_tuple, self.policy_init_value)\n",
        "\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "    def random_policy(self, r, c, vr, vc):\n",
        "        \"\"\"\n",
        "        Get possible actions from the environment and \n",
        "        choose one random action out of the possible actions.\n",
        "\n",
        "        Returns a tuple `(dvr, dvc, action probability)`.\n",
        "        \"\"\"\n",
        "        possible_action_indices = self.env.get_possible_action_indices(vr, vc)\n",
        "        assert not len(possible_action_indices) == 0, f\"No possible actions for {r, c, vr, vc}\"\n",
        "        prob = 1 / len(possible_action_indices)\n",
        "        a_index = np.random.choice(possible_action_indices)\n",
        "        return self.env.all_actions[a_index] + (prob, )\n",
        "\n",
        "    def soft_greedy_policy(self, r, c, vr, vc):\n",
        "        \"\"\"\n",
        "        Get possible actions from the environment, \n",
        "        choose one random action out of the possible actions with \n",
        "        probability `epsilon` or return learned action if it is allowed.\n",
        "\n",
        "        Returns a tuple `(dvr, dvc, action probability)`.\n",
        "        \"\"\"\n",
        "        possible_action_indices = self.env.get_possible_action_indices(vr, vc)\n",
        "        assert not len(possible_action_indices) == 0, f\"No possible actions for {r, c, vr, vc}\"\n",
        "        prob = 1 / len(possible_action_indices)\n",
        "\n",
        "        if np.random.rand() > self.epsilon and self.policy[r, c, vr, vc] in possible_action_indices:\n",
        "            a_index = self.policy[r, c, vr, vc]\n",
        "        else:\n",
        "            a_index = np.random.choice(possible_action_indices)\n",
        "        return self.env.all_actions[a_index] + (prob, )\n",
        "\n",
        "    def greedy_policy(self, r, c, vr, vc):\n",
        "        \"\"\"\n",
        "        Get possible actions from the environment, \n",
        "        choose one random action out of the possible actions\n",
        "         or return learned action if it is allowed.\n",
        "\n",
        "        Returns a tuple `(dvr, dvc, action probability)`.\n",
        "        \"\"\"\n",
        "        possible_action_indices = self.env.get_possible_action_indices(vr, vc)\n",
        "        assert not len(possible_action_indices) == 0, f\"No possible actions for {r, c, vr, vc}\"\n",
        "        prob = 1 / len(possible_action_indices)\n",
        "\n",
        "        if self.policy[r, c, vr, vc] in possible_action_indices:\n",
        "            a_index = self.policy[r, c, vr, vc]\n",
        "        else:\n",
        "            a_index = np.random.choice(possible_action_indices)\n",
        "        return self.env.all_actions[a_index]+ (prob, )"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a91HXgEzC-ur"
      },
      "source": [
        "class MonteCarlo:\n",
        "    def __init__(self, agent, env):\n",
        "        self.gamma = 1\n",
        "        self.agent = agent\n",
        "        self.env = env\n",
        "        self.n_elements = self.agent.policy.size\n",
        "\n",
        "    def learn_episode(self, history):\n",
        "        \"\"\"\n",
        "        Off policy Monte-Carlo control algorithm (p. 111 in Sutton&Barto)\n",
        "        Learn from an episode encoded in `history`.\n",
        "        \"\"\"\n",
        "        G = 0\n",
        "        W = 1\n",
        "        steps_followed_back = 0\n",
        "        for i in range(len(history) - 1, -1, -1):\n",
        "            r, c, vr, vc, d_vr, d_vc, reward, prob = history[i]\n",
        "            a_index = self.env.all_actions_map[f\"{d_vr}_{d_vc}\"]\n",
        "            G = self.gamma * G + reward\n",
        "            self.agent.C[r, c, vr, vc, a_index] += W\n",
        "            self.agent.Q[r, c, vr, vc, a_index] += W / self.agent.C[r, c, vr, vc, a_index] * (\n",
        "                        G - self.agent.Q[r, c, vr, vc, a_index])\n",
        "            self.agent.policy[r, c, vr, vc] = np.argmax(self.agent.Q[r, c, vr, vc])\n",
        "\n",
        "            # NOTE: Learns very slow if next lines are uncommented\n",
        "            # This is a known problem (see Sutton&Barto p. 111 first paragraph)\n",
        "            # if self.agent.policy[r, c, vr, vc] != a_index:\n",
        "            #     break\n",
        "            # W /= prob\n",
        "\n",
        "            steps_followed_back += 1\n",
        "        return steps_followed_back\n",
        "\n",
        "    def optimize(self, train_episodes, eval_episodes):\n",
        "        \"\"\"\n",
        "        Apply Monte-Carlo algorith - generate an episode and learn \n",
        "        from it. Every 1000 episodes evaluate the greedy policy.\n",
        "        \"\"\"\n",
        "        steps_cnt = []\n",
        "        steps_followed_by_algorithm = 0\n",
        "        total_steps = 0\n",
        "\n",
        "        # for ep in tqdm(range(train_episodes)):\n",
        "        for ep in range(train_episodes):\n",
        "            if ep % 1000 == 0:\n",
        "                # Fraction of learned parameters in policy\n",
        "                frac_params = np.count_nonzero(self.agent.policy != self.agent.policy_init_value) / self.n_elements\n",
        "                print(f\"Episodes: {ep}/{train_episodes},  policy: {round(frac_params * 100, 2)}%\")\n",
        "                evaluate_greedy_policy(eval_episodes, self.agent, self.env)\n",
        "                if ep != 0:\n",
        "                    print(f\"Fraction of trajectory steps followed by algorithm: {round(steps_followed_by_algorithm / total_steps * 100, 2)}%\")\n",
        "                    total_steps = 0\n",
        "                    steps_followed_by_algorithm = 0\n",
        "            hist, hits = self.env.get_episode(self.agent.random_policy)\n",
        "            total_steps += len(hist)\n",
        "            steps_cnt.append(len(hist))\n",
        "            steps_followed_by_algorithm += self.learn_episode(hist)\n",
        "\n",
        "\n",
        "def evaluate_greedy_policy(n_episodes, agent, env):\n",
        "    \"\"\"\n",
        "    Generate episodes using greedy policy and compute average number \n",
        "    of times the car hits the boundaries.\n",
        "    \"\"\"\n",
        "    total_hits = 0\n",
        "    for _ in range(n_episodes):\n",
        "        hist, n_hits = env.get_episode(agent.greedy_policy)\n",
        "        total_hits += n_hits\n",
        "    hits_per_episode = total_hits / n_episodes\n",
        "    print(f\"Average number of hits per episode: {round(hits_per_episode, 2)}\")"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qpPtJm_bqYpn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4c1fecf-98b7-4297-8b4c-b5c4f0ab59d4"
      },
      "source": [
        "track_env = Environment()\n",
        "car_agent = Agent(track_env)\n",
        "mc = MonteCarlo(car_agent, track_env)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Building an environment...\n",
            "Start line:\n",
            "x1: 0, y1: 0\n",
            "x2: 10, y2: 0\n",
            "Finish line:\n",
            "x1: 15, y1: 10\n",
            "x2: 15, y2: 30\n",
            "Adding boundary...\n",
            "x1: 10, y1: 0\n",
            "x2: 10, y2: 10\n",
            "Adding boundary...\n",
            "x1: 10, y1: 10\n",
            "x2: 15, y2: 10\n",
            "Adding boundary...\n",
            "x1: 15, y1: 30\n",
            "x2: 0, y2: 30\n",
            "Adding boundary...\n",
            "x1: 0, y1: 30\n",
            "x2: 0, y2: 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W1WRlkzNrR0c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cec200e8-9c4f-4c8c-a848-f44412e0d7bc"
      },
      "source": [
        "mc.optimize(20000, 1000)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Episodes: 0/20000,  policy: 10.59%\n",
            "Average number of hits per episode: 9.73\n",
            "Episodes: 1000/20000,  policy: 10.62%\n",
            "Average number of hits per episode: 9.43\n",
            "Fraction of trajectory steps followed by algorithm: 0.07%\n",
            "Episodes: 2000/20000,  policy: 10.66%\n",
            "Average number of hits per episode: 9.72\n",
            "Fraction of trajectory steps followed by algorithm: 0.07%\n",
            "Episodes: 3000/20000,  policy: 10.69%\n",
            "Average number of hits per episode: 9.33\n",
            "Fraction of trajectory steps followed by algorithm: 0.09%\n",
            "Episodes: 4000/20000,  policy: 10.72%\n",
            "Average number of hits per episode: 10.02\n",
            "Fraction of trajectory steps followed by algorithm: 0.06%\n",
            "Episodes: 5000/20000,  policy: 10.77%\n",
            "Average number of hits per episode: 9.84\n",
            "Fraction of trajectory steps followed by algorithm: 0.08%\n",
            "Episodes: 6000/20000,  policy: 10.82%\n",
            "Average number of hits per episode: 9.74\n",
            "Fraction of trajectory steps followed by algorithm: 0.06%\n",
            "Episodes: 7000/20000,  policy: 10.84%\n",
            "Average number of hits per episode: 9.76\n",
            "Fraction of trajectory steps followed by algorithm: 0.1%\n",
            "Episodes: 8000/20000,  policy: 10.86%\n",
            "Average number of hits per episode: 9.29\n",
            "Fraction of trajectory steps followed by algorithm: 0.06%\n",
            "Episodes: 9000/20000,  policy: 10.9%\n",
            "Average number of hits per episode: 10.4\n",
            "Fraction of trajectory steps followed by algorithm: 0.08%\n",
            "Episodes: 10000/20000,  policy: 10.92%\n",
            "Average number of hits per episode: 10.41\n",
            "Fraction of trajectory steps followed by algorithm: 0.07%\n",
            "Episodes: 11000/20000,  policy: 10.94%\n",
            "Average number of hits per episode: 9.93\n",
            "Fraction of trajectory steps followed by algorithm: 0.06%\n",
            "Episodes: 12000/20000,  policy: 10.94%\n",
            "Average number of hits per episode: 9.65\n",
            "Fraction of trajectory steps followed by algorithm: 0.05%\n",
            "Episodes: 13000/20000,  policy: 11.0%\n",
            "Average number of hits per episode: 9.43\n",
            "Fraction of trajectory steps followed by algorithm: 0.09%\n",
            "Episodes: 14000/20000,  policy: 11.01%\n",
            "Average number of hits per episode: 9.82\n",
            "Fraction of trajectory steps followed by algorithm: 0.06%\n",
            "Episodes: 15000/20000,  policy: 11.02%\n",
            "Average number of hits per episode: 9.98\n",
            "Fraction of trajectory steps followed by algorithm: 0.07%\n",
            "Episodes: 16000/20000,  policy: 11.03%\n",
            "Average number of hits per episode: 10.12\n",
            "Fraction of trajectory steps followed by algorithm: 0.07%\n",
            "Episodes: 17000/20000,  policy: 11.05%\n",
            "Average number of hits per episode: 9.83\n",
            "Fraction of trajectory steps followed by algorithm: 0.06%\n",
            "Episodes: 18000/20000,  policy: 11.09%\n",
            "Average number of hits per episode: 9.96\n",
            "Fraction of trajectory steps followed by algorithm: 0.09%\n",
            "Episodes: 19000/20000,  policy: 11.1%\n",
            "Average number of hits per episode: 9.58\n",
            "Fraction of trajectory steps followed by algorithm: 0.05%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KFURqpyBvRsQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        },
        "outputId": "2e330f39-dab3-42be-f254-5970d93a0544"
      },
      "source": [
        "history, hits = track_env.get_episode(car_agent.greedy_policy)\n",
        "print(f\"Number of steps: {len(history)}, number of hits: {hits}\")\n",
        "track_env.plot_episode(history)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of steps: 11, number of hits: 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Yx_kkZtvN1H"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}