{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "racetrack_monte_carlo_chapter5_v3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMuHlEoDu+0xkPKDEn9t1to",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ychervonyi/reinforcement-learning-learning/blob/main/racetrack_monte_carlo_chapter5_v3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gMRc-0RVDUxU"
      },
      "source": [
        "import numpy as np\n",
        "from collections import Counter\n",
        "from matplotlib import pyplot\n",
        "from tqdm import tqdm\n",
        "\n",
        "np.random.seed(1231231)\n",
        "\n",
        "\n",
        "class Point:\n",
        "    def __init__(self, rr, cc):\n",
        "        self.y = rr\n",
        "        self.x = cc\n",
        "\n",
        "\n",
        "def collinear(p1, p2, p3):\n",
        "    \"\"\"\n",
        "    Return true iff p1, p2, and p3 all lie on the same line.\n",
        "    \"\"\"\n",
        "    return (p2.x - p1.x) * (p3.y - p1.y) == (p3.x - p1.x) * (p2.y - p1.y)\n",
        "\n",
        "\n",
        "def within(a, b, c):\n",
        "    \"\"\"\n",
        "    Return true iff b is between a and c (inclusive).\n",
        "    \"\"\"\n",
        "    return a <= b <= c or c <= b <= a\n",
        "\n",
        "\n",
        "class Segment:\n",
        "    def __init__(self, p1, p2):\n",
        "        self.p1 = p1\n",
        "        self.p2 = p2\n",
        "        self.A = p2.y - p1.y\n",
        "        self.B = p1.x - p2.x\n",
        "        self.C = self.A * p1.x + self.B * p1.y\n",
        "\n",
        "    def print(self):\n",
        "        print(f\"x1: {self.p1.x}, y1: {self.p1.y}\")\n",
        "        print(f\"x2: {self.p2.x}, y2: {self.p2.y}\")\n",
        "\n",
        "    def p_is_on(self, p):\n",
        "        return (collinear(self.p1, self.p2, p)\n",
        "                and (within(self.p1.x, p.x, self.p2.x) if self.p1.x != self.p2.x else\n",
        "                     within(self.p1.y, p.y, self.p2.y)))\n",
        "\n",
        "\n",
        "def segments_intersect(s1, s2):\n",
        "    A1, B1, C1 = s1.A, s1.B, s1.C\n",
        "    A2, B2, C2 = s2.A, s2.B, s2.C\n",
        "    det = A1 * B2 - A2 * B1\n",
        "    if det == 0:\n",
        "        return False\n",
        "    x = (B2 * C1 - B1 * C2) / det\n",
        "    y = (A1 * C2 - A2 * C1) / det\n",
        "\n",
        "    for s in (s1, s2):\n",
        "        x1, x2 = s.p1.x, s.p2.x\n",
        "        y1, y2 = s.p1.y, s.p2.y\n",
        "        if not (min(x1, x2) <= x <= max(x1, x2) and min(y1, y2) <= y <= max(y1, y2)):\n",
        "            return False\n",
        "    return True\n",
        "\n",
        "\n",
        "def compare_points(p1, p2):\n",
        "    return p1.x == p2.x and p1.y == p2.y\n",
        "\n",
        "\n",
        "def compare_segments(s1, s2):\n",
        "    return compare_points(s1.p1, s2.p1) and compare_points(s1.p2, s2.p2)\n",
        "\n",
        "\n",
        "class Environment:\n",
        "    def __init__(self):\n",
        "        self.build()\n",
        "\n",
        "    def build(self):\n",
        "        # NOTE: this is an ugly way to define the track geometry\n",
        "        # Here we define the track as a polygon\n",
        "        rows = 30\n",
        "        cols = 15\n",
        "\n",
        "        self.max_v, self.min_v = 5, 0\n",
        "        self.rows, self.cols = rows, cols\n",
        "        self.state_tuple = (rows, cols, self.max_v, self.max_v)\n",
        "\n",
        "        self.start_segment = Segment(Point(0, 0), Point(0, 10))\n",
        "        print(\"Start line:\")\n",
        "        self.start_segment.print()\n",
        "        self.finish_segment = Segment(Point(10, 15), Point(30, 15))\n",
        "        print(\"Finish line:\")\n",
        "        self.finish_segment.print()\n",
        "\n",
        "        # Define start points\n",
        "        self.start_points = []\n",
        "        r = 0\n",
        "        for c in range(1, 10):\n",
        "            self.start_points.append((r, c))\n",
        "\n",
        "        # Boundaries\n",
        "        points = [\n",
        "            Point(0, 0),\n",
        "            Point(0, 10),\n",
        "            Point(10, 10),\n",
        "            Point(10, 15),\n",
        "            Point(30, 15),\n",
        "            Point(30, 0)\n",
        "        ]\n",
        "\n",
        "        # Bounds should not have start and finish lines\n",
        "        self.bounds = []\n",
        "        for i in range(1, len(points)):\n",
        "            s = Segment(points[i - 1], points[i])\n",
        "            if compare_segments(s, self.finish_segment) or compare_segments(s, self.start_segment):\n",
        "                continue\n",
        "            print(f\"Adding boundary...\")\n",
        "            s.print()\n",
        "            self.bounds.append(s)\n",
        "\n",
        "        self.bounds.append(Segment(points[-1], points[0]))\n",
        "        print(f\"Adding boundary...\")\n",
        "        self.bounds[-1].print()\n",
        "\n",
        "        self.build_all_actions()\n",
        "\n",
        "    def build_all_actions(self):\n",
        "        dv = [-1, 0, 1]\n",
        "        n_dv = len(dv)\n",
        "        self.all_actions_map = {}\n",
        "        self.all_actions = []\n",
        "        for i, d_vr in enumerate(dv):\n",
        "            for j, d_vc in enumerate(dv):\n",
        "                self.all_actions.append((d_vr, d_vc))\n",
        "                self.all_actions_map[f\"{d_vr}_{d_vc}\"] = i * n_dv + j\n",
        "        self.n_actions = len(self.all_actions)\n",
        "\n",
        "    def is_good_velocity(self, vr_, vc_):\n",
        "        if (vr_, vc_) == (\n",
        "        self.min_v, self.min_v) or vr_ < self.min_v or vr_ >= self.max_v or vc_ < self.min_v or vc_ >= self.max_v:\n",
        "            return False\n",
        "        return True\n",
        "\n",
        "    def get_possible_action_indices(self, vr_, vc_):\n",
        "        \"\"\"\n",
        "        Get possible actions for current velocity `vr_, vc_`.\n",
        "        \"\"\"\n",
        "        # 9 actions for each state - increase, decrease or\n",
        "        # don't change velocity components\n",
        "\n",
        "        possible_actions_indicies = []\n",
        "        for d_vr, d_vc in self.all_actions:\n",
        "            if self.is_good_velocity(vr_ + d_vr, vc_ + d_vc):\n",
        "                possible_actions_indicies.append(self.all_actions_map[f\"{d_vr}_{d_vc}\"])\n",
        "        return possible_actions_indicies\n",
        "\n",
        "    def get_episode(self, policy_fn):\n",
        "        \"\"\"\n",
        "        Generate an episode.\n",
        "        \"\"\"\n",
        "        hits = 0\n",
        "        reward = -1\n",
        "        vr = vc = 0\n",
        "        r, c = self.start_points[np.random.randint(0, len(self.start_points))]\n",
        "        finish = False\n",
        "        history = []\n",
        "        while not finish:\n",
        "            d_vr, d_vc, prob = policy_fn(r, c, vr, vc)\n",
        "            new_vr, new_vc = vr + d_vr, vc + d_vc\n",
        "\n",
        "            assert not (new_vr == 0 and new_vc == 0 and (r, c) in self.start_points), f\"{new_vr} {new_vc} {r} {c}\"\n",
        "\n",
        "            # Check if the new path intersects finish line\n",
        "            # If yes we are done with this episode\n",
        "            new_path = Segment(Point(r, c), Point(r + new_vr, c + new_vc))\n",
        "            if segments_intersect(self.finish_segment, new_path):\n",
        "                history.append((r, c, vr, vc, d_vr, d_vc, 0, prob))\n",
        "                break\n",
        "\n",
        "            # Check if the path intersects boundaries\n",
        "            # If yes we move the car to the start line\n",
        "            hit = False\n",
        "            for bound in self.bounds:\n",
        "                if segments_intersect(bound, new_path):\n",
        "                    hit = True\n",
        "                    # TODO: Worse reward for hitting a boundary?\n",
        "                    history.append((r, c, vr, vc, d_vr, d_vc, -5, prob))\n",
        "                    hits += 1\n",
        "                    r, c = self.start_points[np.random.randint(0, len(self.start_points))]\n",
        "                    vr = vc = 0\n",
        "\n",
        "            # Update velocity and position if didn't hit a bound\n",
        "            if not hit:\n",
        "                history.append((r, c, vr, vc, d_vr, d_vc, -1, prob))\n",
        "                vr, vc = new_vr, new_vc\n",
        "                r += vr\n",
        "                c += vc\n",
        "        return history, hits\n",
        "\n",
        "    def plot_episode(self, history):\n",
        "        track = np.zeros((self.rows, self.cols))\n",
        "        pyplot.imshow(track)\n",
        "        for item in history:\n",
        "            # r, c, vr, vc, d_vr, d_vc, reward, p = item\n",
        "            pyplot.plot(item[1], item[0], '.r')\n",
        "        pyplot.show()\n",
        "\n",
        "    def print_episode(self, history):\n",
        "        \"\"\"\n",
        "        Follow `history` and print how many times in history\n",
        "        each cell was visited.\n",
        "        \"\"\"\n",
        "        track = np.zeros((self.rows, self.cols))\n",
        "        for item in history:\n",
        "            # r, c, vr, vc, d_vr, d_vc, reward, p = item\n",
        "            track[item[0], item[1]] += 1\n",
        "        return track.astype(\"int\")"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EwtNOBzhDY8V"
      },
      "source": [
        "class Agent:\n",
        "    def __init__(self, env, epsilon=0.1):\n",
        "        self.env = env\n",
        "        # Value function\n",
        "        self.Q = np.random.uniform(\n",
        "            low=0,\n",
        "            high=self.env.n_actions,\n",
        "            size=self.env.state_tuple + (self.env.n_actions,)\n",
        "        )\n",
        "        # self.Q = np.zeros(self.env.state_tuple + (self.env.n_actions,))\n",
        "\n",
        "        # Cumulative sum of weights\n",
        "        self.C = np.zeros(self.env.state_tuple + (self.env.n_actions,))\n",
        "        # Policy\n",
        "        # self.policy = np.argmax(Q, axis=4)\n",
        "        # self.policy = np.zeros((rows, cols))\n",
        "        self.policy_init_value = -1\n",
        "        self.policy = np.full(self.env.state_tuple, self.policy_init_value)\n",
        "\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "    def random_policy(self, r, c, vr, vc):\n",
        "        possible_action_indices = self.env.get_possible_action_indices(vr, vc)\n",
        "        assert not len(possible_action_indices) == 0, f\"No possible actions for {r, c, vr, vc}\"\n",
        "        prob = 1 / len(possible_action_indices)\n",
        "        a_index = np.random.choice(possible_action_indices)\n",
        "        return self.env.all_actions[a_index] + (prob, )\n",
        "\n",
        "    def soft_greedy_policy(self, r, c, vr, vc):\n",
        "        possible_action_indices = self.env.get_possible_action_indices(vr, vc)\n",
        "        assert not len(possible_action_indices) == 0, f\"No possible actions for {r, c, vr, vc}\"\n",
        "        prob = 1 / len(possible_action_indices)\n",
        "\n",
        "        if np.random.rand() > self.epsilon and self.policy[r, c, vr, vc] in possible_action_indices:\n",
        "            a_index = self.policy[r, c, vr, vc]\n",
        "        else:\n",
        "            a_index = np.random.choice(possible_action_indices)\n",
        "        return self.env.all_actions[a_index] + (prob, )\n",
        "\n",
        "    def greedy_policy(self, r, c, vr, vc):\n",
        "        possible_action_indices = self.env.get_possible_action_indices(vr, vc)\n",
        "        assert not len(possible_action_indices) == 0, f\"No possible actions for {r, c, vr, vc}\"\n",
        "        prob = 1 / len(possible_action_indices)\n",
        "\n",
        "        if self.policy[r, c, vr, vc] in possible_action_indices:\n",
        "            a_index = self.policy[r, c, vr, vc]\n",
        "        else:\n",
        "            a_index = np.random.choice(possible_action_indices)\n",
        "        return self.env.all_actions[a_index]+ (prob, )"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a91HXgEzC-ur"
      },
      "source": [
        "class MonteCarlo:\n",
        "    def __init__(self, agent, env):\n",
        "        self.gamma = 1\n",
        "        self.agent = agent\n",
        "        self.env = env\n",
        "        self.n_elements = self.agent.policy.size\n",
        "\n",
        "    def learn_episode(self, history):\n",
        "        \"\"\"\n",
        "        Learn from an episode\n",
        "        \"\"\"\n",
        "        G = 0\n",
        "        W = 1\n",
        "        for i in range(len(history) - 1, -1, -1):\n",
        "            r, c, vr, vc, d_vr, d_vc, reward, prob = history[i]\n",
        "            a_index = self.env.all_actions_map[f\"{d_vr}_{d_vc}\"]\n",
        "            G = self.gamma * G + reward\n",
        "            self.agent.C[r, c, vr, vc, a_index] += W\n",
        "            self.agent.Q[r, c, vr, vc, a_index] += W / self.agent.C[r, c, vr, vc, a_index] * (\n",
        "                        G - self.agent.Q[r, c, vr, vc, a_index])\n",
        "            self.agent.policy[r, c, vr, vc] = np.argmax(self.agent.Q[r, c, vr, vc])\n",
        "\n",
        "            if self.agent.policy[r, c, vr, vc] != a_index:\n",
        "                break\n",
        "            W /= prob\n",
        "\n",
        "    def optimize(self, n_episodes):\n",
        "        steps_cnt = []\n",
        "\n",
        "        for ep in tqdm(range(n_episodes)):\n",
        "            if ep % 1000 == 0:\n",
        "                # Fraction of learned parameters in policy\n",
        "                frac_params = np.count_nonzero(self.agent.policy != self.agent.policy_init_value) / self.n_elements\n",
        "                print(f\"Episode: {ep},  policy: {round(frac_params * 100, 2)}%\")\n",
        "            hist, hits = self.env.get_episode(self.agent.random_policy)\n",
        "            steps_cnt.append(len(hist))\n",
        "            self.learn_episode(hist)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qpPtJm_bqYpn",
        "outputId": "cce9d3f5-f1f6-419d-f5de-46a3523ba836",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "env = Environment()\n",
        "agent = Agent(env)\n",
        "mc = MonteCarlo(agent, env)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start line:\n",
            "x1: 0, y1: 0\n",
            "x2: 10, y2: 0\n",
            "Finish line:\n",
            "x1: 15, y1: 10\n",
            "x2: 15, y2: 30\n",
            "Adding boundary...\n",
            "x1: 10, y1: 0\n",
            "x2: 10, y2: 10\n",
            "Adding boundary...\n",
            "x1: 10, y1: 10\n",
            "x2: 15, y2: 10\n",
            "Adding boundary...\n",
            "x1: 15, y1: 30\n",
            "x2: 0, y2: 30\n",
            "Adding boundary...\n",
            "x1: 0, y1: 30\n",
            "x2: 0, y2: 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W1WRlkzNrR0c",
        "outputId": "47c7c2b2-e47c-4a06-bf0b-af65657d2155",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "mc.optimize(20000)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  1%|          | 147/20000 [00:00<00:29, 664.14it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Episode: 0,  policy: 0.0%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  6%|▌         | 1229/20000 [00:01<00:24, 769.90it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Episode: 1000,  policy: 4.39%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 11%|█         | 2158/20000 [00:02<00:22, 793.28it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Episode: 2000,  policy: 5.82%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 16%|█▌        | 3167/20000 [00:04<00:20, 834.84it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Episode: 3000,  policy: 6.49%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 21%|██        | 4222/20000 [00:05<00:19, 829.66it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Episode: 4000,  policy: 6.84%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 26%|██▌       | 5118/20000 [00:06<00:19, 779.21it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Episode: 5000,  policy: 7.16%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 31%|███       | 6155/20000 [00:07<00:19, 705.22it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Episode: 6000,  policy: 7.47%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 36%|███▌      | 7201/20000 [00:09<00:16, 781.91it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Episode: 7000,  policy: 7.64%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 41%|████      | 8108/20000 [00:10<00:14, 808.75it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Episode: 8000,  policy: 7.87%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 46%|████▌     | 9199/20000 [00:11<00:13, 778.29it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Episode: 9000,  policy: 8.02%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 51%|█████     | 10218/20000 [00:13<00:13, 707.26it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Episode: 10000,  policy: 8.25%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 55%|█████▌    | 11087/20000 [00:14<00:12, 737.13it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Episode: 11000,  policy: 8.36%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 61%|██████    | 12149/20000 [00:15<00:10, 758.53it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Episode: 12000,  policy: 8.51%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 66%|██████▌   | 13119/20000 [00:17<00:09, 759.62it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Episode: 13000,  policy: 8.6%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 71%|███████   | 14203/20000 [00:18<00:07, 763.69it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Episode: 14000,  policy: 8.76%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 76%|███████▌  | 15139/20000 [00:19<00:05, 819.14it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Episode: 15000,  policy: 8.85%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 81%|████████  | 16203/20000 [00:20<00:04, 779.28it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Episode: 16000,  policy: 8.98%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 86%|████████▌ | 17190/20000 [00:22<00:03, 704.31it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Episode: 17000,  policy: 9.09%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 91%|█████████ | 18216/20000 [00:23<00:02, 807.32it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Episode: 18000,  policy: 9.2%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 96%|█████████▌| 19114/20000 [00:24<00:01, 822.41it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Episode: 19000,  policy: 9.29%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 20000/20000 [00:26<00:00, 767.53it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KFURqpyBvRsQ",
        "outputId": "bb93c5c1-23f0-4fab-c797-03e75d9e8b6c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        }
      },
      "source": [
        "history, hits = env.get_episode(agent.greedy_policy)\n",
        "print(f\"Number of steps: {len(history)}, number of hits: {hits}\")\n",
        "env.plot_episode(history)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of steps: 12, number of hits: 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5aOqAxhDJXQU",
        "outputId": "09e57438-65d9-427e-d19a-af66fe329675",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        }
      },
      "source": [
        "history, hits = env.get_episode(agent.greedy_policy)\n",
        "print(f\"Number of steps: {len(history)}, number of hits: {hits}\")\n",
        "env.plot_episode(history)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of steps: 21, number of hits: 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VYRyLnc-_qki",
        "outputId": "7cefede1-5caa-4f6a-cdeb-7d34cc1f9d4f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        }
      },
      "source": [
        "history, hits = env.get_episode(agent.greedy_policy)\n",
        "print(f\"Number of steps: {len(history)}, number of hits: {hits}\")\n",
        "env.plot_episode(history)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of steps: 10, number of hits: 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aGes0yPX_s4E"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}