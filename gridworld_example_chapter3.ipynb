{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "gridworld_example_chapter3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPJ6yQUz/sqwxzGshFfLKho",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ychervonyi/reinforcement-learning-learning/blob/main/gridworld_example_chapter3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H062fqhJnxyL"
      },
      "source": [
        "Bellman optimality equation for the value function is $v_*(s) = \\mathrm{max}_a\\sum_{s', r'} p(s', r|s, a)[r+\\gamma v_*(s')]$. $s$ - any state, $s'$ - successor state, $r$ - rewards, $a$ - actions. $p(s', r|s, a)$ defines the dynamics of Markov decision processes $p(s', r|s, a) = \\mathrm{Pr}(S_t=s', R_t=r| S_{t-1}=s,A_{t-1}=a)$.\n",
        "\n",
        "These equations can be used to find the optimal policy. One way to solve them is using dynamic programming (see Chapters 3 and 4 in Sutton & Barto). In this notebook we will use some of the dynamic programming methods to solve grid world example (p. 60, example 3.5)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WKWlAix-p4Oa"
      },
      "source": [
        "In the gridworld example there are 25 states (5x5 grid) $\\mathcal{S} = \\{0, 1..., 23, 24\\}$. There are 4 actions for each cell - left, right, up, down $\\mathcal{A}(s) = \\{0, 1, 2, 3\\}$ for all $s$. There are 4 rewards $\\mathcal{R}\\in \\{-1, 0, 10, 5\\}$.\n",
        "\n",
        "Let's define all $p(s',r|s,a)$. \n",
        "For example, for state 0 (upper left corner) we get \n",
        "\\begin{equation}\n",
        "p(s',r|0,left) = \\left\\{ \\begin{matrix}1 ~ \\mathrm{if}~s'=0~\\mathrm{and}~r=-1  \\\\ 0~\\mathrm{otherwise} \\end{matrix}\\right.\\\\\n",
        "p(s',r|0,up) = \\left\\{ \\begin{matrix}1 ~ \\mathrm{if}~s'=0~\\mathrm{and}~r=-1  \\\\ 0~\\mathrm{otherwise} \\end{matrix}\\right.\\\\\n",
        "p(s',r|0,right) = \\left\\{ \\begin{matrix}1 ~ \\mathrm{if}~s'=1~\\mathrm{and}~r=0  \\\\ 0~\\mathrm{otherwise} \\end{matrix}\\right.\\\\\n",
        "p(s',r|0,down) = \\left\\{ \\begin{matrix}1 ~ \\mathrm{if}~s'=5~\\mathrm{and}~r=0  \\\\ 0~\\mathrm{otherwise} \\end{matrix}\\right.\\\\\n",
        "\\end{equation}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8REuU_9inpaF"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "side = 5\n",
        "states = np.array([[side * c + r for r in range(side)] for c in range(side)]) # coordinates [r, c]\n",
        "rewards = {-1: 0, 0: 1, 10: 2, 5: 3} # [-1, 0, 10, 5]\n",
        "actions = [[-1, 0], [1, 0], [0, -1], [0, 1]] # [dr, dc] = [up, down, left, right]\n",
        "actions_map = [\"\\u2191\", \"\\u2193\", \"\\u2190\", \"\\u2192\"]\n",
        "\n",
        "p = np.zeros((states.size, len(rewards), states.size, len(actions)))\n",
        "\n",
        "# Assign p(s_p, r| s, a) for all cells except A and B\n",
        "for r in range(side):\n",
        "    for c in range(side):\n",
        "        for i_a, (dr, dc) in enumerate(actions):\n",
        "            # Skip positions A and B (these are special cases - see lower)\n",
        "            if (r, c) in ((0, 1), (0, 3)):\n",
        "                continue\n",
        "            # New state\n",
        "            r_p, c_p = r + dr, c + dc\n",
        "            # Edge - return to previous state and assing reward -1\n",
        "            if r_p in (-1, side) or c_p in (-1, side):\n",
        "                p[states[r][c]][rewards[-1]][states[r][c]][i_a] = 1\n",
        "            # Non-edge (inside the grid) - reward 0\n",
        "            else:\n",
        "                p[states[r_p][c_p]][rewards[0]][states[r][c]][i_a] = 1\n",
        "# A -> A' with reward +10\n",
        "p[states[4][1], rewards[10], states[0][1], :] = np.ones(len(actions))\n",
        "# B -> B' with reward +5\n",
        "p[states[2][3], rewards[5], states[0][3], :] = np.ones(len(actions))"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JizY96EDwuTD",
        "outputId": "4e8e1c91-428d-48d3-b200-2cdbd7f010d0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Check 1: final state (0,0), reward -1, original state (0, 0). For left and up we should get probabilities 1\n",
        "p[states[0][0], rewards[-1], states[0][0], ::]"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1., 0., 1., 0.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XvTD0ro1rhEP"
      },
      "source": [
        "def print_mtr(A):\n",
        "    \"\"\"\n",
        "    Utility to print a square matrix of size (side, side).\n",
        "    \"\"\"\n",
        "    for r in range(side):\n",
        "        s = \"\"\n",
        "        t = isinstance(A, (np.ndarray))\n",
        "        for c in range(side):\n",
        "            item = A[r*side+c]\n",
        "            if t:\n",
        "                item = round(item, 1)\n",
        "            s += f\" {item}\"\n",
        "        print(s)\n",
        "\n",
        "def iterative_policy_evaluation(gamma, epsilon):\n",
        "    \"\"\"\n",
        "    Iterative policy evaluation (p. 75)\n",
        "    \"\"\"\n",
        "\n",
        "    # Value function\n",
        "    V = np.random.uniform(low=-1, high=1, size=states.size)\n",
        "    # V = np.zeros(states.size)\n",
        "\n",
        "    steps, max_steps = 0, 20\n",
        "    # Initial delta\n",
        "    delta = epsilon\n",
        "    # Keep max_steps in case it does not converge\n",
        "    while delta >= epsilon and steps < max_steps:\n",
        "        # Find max delta at each iteration\n",
        "        delta = 0\n",
        "        for s in range(states.size):\n",
        "            v = V[s]\n",
        "            v_new = 0\n",
        "            for a_i in range(len(actions)):\n",
        "                tmp = 0\n",
        "                for s_p in range(states.size):\n",
        "                    for r, r_i in rewards.items():\n",
        "                        tmp += p[s_p][r_i][s][a_i] * (r + gamma * V[s_p])\n",
        "                v_new = max(v_new, tmp)\n",
        "            delta = max(delta, abs(v - v_new))\n",
        "            V[s] = v_new\n",
        "        print(f\"delta: {delta}\")\n",
        "        # print_mtr(greedy_optimal_policy(V))\n",
        "        # print_v(V)\n",
        "        steps += 1\n",
        "    print(\"Value function\")\n",
        "    print_mtr(V)\n",
        "\n",
        "    # Print greedy optimal policy\n",
        "    print(\"Optimal action\")\n",
        "    optimal_policy = greedy_optimal_policy(V)\n",
        "    print_mtr(optimal_policy)\n",
        "    return V, optimal_policy\n",
        "\n",
        "def greedy_optimal_policy(value_func):\n",
        "    \"\"\"\n",
        "    Find optimal action for each state (optimal policy).\n",
        "    We find ONE optimal action for simplicity\n",
        "    (even though there could be many)\n",
        "    \"\"\"\n",
        "    n = len(value_func)\n",
        "    optimal_actions = [0] * n\n",
        "    for s in range(n):\n",
        "        max_v = 0\n",
        "        for a_i in range(len(actions)):\n",
        "            tmp = 0\n",
        "            for s_p in range(states.size):\n",
        "                for r, r_i in rewards.items():\n",
        "                    tmp += p[s_p][r_i][s][a_i] * (r + gamma * value_func[s_p])\n",
        "            if tmp > max_v:\n",
        "                max_v = tmp\n",
        "                optimal_actions[s] = actions_map[a_i]\n",
        "    return optimal_actions"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S8prWVo61n_C",
        "outputId": "31d90064-a2fa-48cf-d1b7-c15a3903f1b2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "gamma = 0.5\n",
        "threshold = 0.01\n",
        "print(f\"gamma: {gamma}, epsilon: {threshold}\")\n",
        "vv, aa = iterative_policy_evaluation(gamma, threshold)\n",
        "\n",
        "print(\"=====================\")\n",
        "gamma = 0.9\n",
        "print(f\"gamma: {gamma}, epsilon: {threshold}\")\n",
        "_, _ = iterative_policy_evaluation(gamma, threshold)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "gamma: 0.5, epsilon: 0.01\n",
            "delta: 9.747184066025968\n",
            "delta: 4.873592033012984\n",
            "delta: 0.18455835642333174\n",
            "delta: 0.011936055982569194\n",
            "delta: 0.0014920069978208161\n",
            "Value function\n",
            " 5.2 10.3 5.2 5.7 2.9\n",
            " 2.6 5.2 2.6 2.9 1.4\n",
            " 1.3 2.6 1.3 1.4 0.7\n",
            " 0.6 1.3 0.6 0.7 0.4\n",
            " 0.3 0.6 0.3 0.4 0.2\n",
            "Optimal action\n",
            " → ↑ ← ↑ ←\n",
            " → ↑ ↑ ↑ ↑\n",
            " → ↑ ↑ ↑ ↑\n",
            " → ↑ ↑ ↑ ↑\n",
            " → ↑ ↑ ↑ ↑\n",
            "=====================\n",
            "gamma: 0.9, epsilon: 0.01\n",
            "delta: 9.760345224343439\n",
            "delta: 8.784310701909096\n",
            "delta: 5.387548205166979\n",
            "delta: 3.1812933396690486\n",
            "delta: 1.8785219041411771\n",
            "delta: 1.1092483991763267\n",
            "delta: 0.6550000872296273\n",
            "delta: 0.3867710015082224\n",
            "delta: 0.2283844086805935\n",
            "delta: 0.1348587094818008\n",
            "delta: 0.07963271936190708\n",
            "delta: 0.04702232445601595\n",
            "delta: 0.027766212368032228\n",
            "delta: 0.016395670741196966\n",
            "delta: 0.009681479615966992\n",
            "Value function\n",
            " 22.0 24.4 22.0 19.4 17.5\n",
            " 19.8 22.0 19.8 17.8 16.0\n",
            " 17.8 19.8 17.8 16.0 14.4\n",
            " 16.0 17.8 16.0 14.4 13.0\n",
            " 14.4 16.0 14.4 13.0 11.7\n",
            "Optimal action\n",
            " → ↑ ← ↑ ←\n",
            " → ↑ ↑ ← ←\n",
            " → ↑ ↑ ↑ ↑\n",
            " → ↑ ↑ ↑ ↑\n",
            " → ↑ ↑ ↑ ↑\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UXNN6DqhHtoJ",
        "outputId": "4d7032ff-d676-4136-a93b-65cd30d4af9b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Let's check that there some states with multiple optimal actions\n",
        "s = 1\n",
        "for a_i in range(len(actions)):\n",
        "    tmp = 0\n",
        "    for s_p in range(states.size):\n",
        "        for r, r_i in rewards.items():\n",
        "            tmp += p[s_p][r_i][s][a_i] * (r + gamma * vv[s_p])\n",
        "    print(tmp)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10.580645140850619\n",
            "10.580645140850619\n",
            "10.580645140850619\n",
            "10.580645140850619\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXNuUF_eHdbX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}