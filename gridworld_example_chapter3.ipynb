{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "gridworld_example_chapter3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPLqVCYa2TmhTpt2Qgs9Yjl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ychervonyi/reinforcement-learning-learning/blob/main/gridworld_example_chapter3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H062fqhJnxyL"
      },
      "source": [
        "Bellman optimality equation for the value function is $v_*(s) = \\mathrm{max}_a\\sum_{s', r'} p(s', r|s, a)[r+\\gamma v_*(s')]$. $s$ - any state, $s'$ - successor state, $r$ - rewards, $a$ - actions. $p(s', r|s, a)$ defines the dynamics of Markov decision processes $p(s', r|s, a) = \\mathrm{Pr}(S_t=s', R_t=r| S_{t-1}=s,A_{t-1}=a)$.\n",
        "\n",
        "These equations can be used to find the optimal policy. One way to solve them is using dynamic programming (see Chapters 3 and 4 in Sutton & Barto). In this notebook we will use some of the dynamic programming methods to solve grid world example (p. 60, example 3.5)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WKWlAix-p4Oa"
      },
      "source": [
        "In the gridworld example there are 25 states (5x5 grid) $\\mathcal{S} = \\{0, 1..., 23, 24\\}$. There are 4 actions for each cell - left, right, up, down $\\mathcal{A}(s) = \\{0, 1, 2, 3\\}$ for all $s$. There are 4 rewards $\\mathcal{R}\\in \\{-1, 0, 10, 5\\}$.\n",
        "\n",
        "Let's define all $p(s',r|s,a)$. \n",
        "For example, for state 0 (upper left corner) we get \n",
        "\\begin{equation}\n",
        "p(s',r|0,left) = \\left\\{ \\begin{matrix}1 ~ \\mathrm{if}~s'=0~\\mathrm{and}~r=-1  \\\\ 0~\\mathrm{otherwise} \\end{matrix}\\right.\\\\\n",
        "p(s',r|0,up) = \\left\\{ \\begin{matrix}1 ~ \\mathrm{if}~s'=0~\\mathrm{and}~r=-1  \\\\ 0~\\mathrm{otherwise} \\end{matrix}\\right.\\\\\n",
        "p(s',r|0,right) = \\left\\{ \\begin{matrix}1 ~ \\mathrm{if}~s'=1~\\mathrm{and}~r=0  \\\\ 0~\\mathrm{otherwise} \\end{matrix}\\right.\\\\\n",
        "p(s',r|0,down) = \\left\\{ \\begin{matrix}1 ~ \\mathrm{if}~s'=5~\\mathrm{and}~r=0  \\\\ 0~\\mathrm{otherwise} \\end{matrix}\\right.\\\\\n",
        "\\end{equation}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8REuU_9inpaF"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "side = 5\n",
        "states = np.array([[side * c + r for r in range(side)] for c in range(side)]) # coordinates [r, c]\n",
        "rewards = {-1: 0, 0: 1, 10: 2, 5: 3} # [-1, 0, 10, 5]\n",
        "actions = [[-1, 0], [1, 0], [0, -1], [0, 1]] # [dr, dc] = [up, down, left, right]\n",
        "\n",
        "p = np.zeros((states.size, len(rewards), states.size, len(actions)))\n",
        "\n",
        "# Assign p(s_p, r| s, a) for edges and all non-special cells\n",
        "for r in range(side):\n",
        "    for c in range(side):\n",
        "        for i_a, (dr, dc) in enumerate(actions):\n",
        "            # New state\n",
        "            r_p, c_p = r + dr, c + dc\n",
        "            # Edge - return to previous state and assing reward -1\n",
        "            if r_p in (-1, side) or c_p in (-1, side):\n",
        "                p[states[r][c]][rewards[-1]][states[r][c]][i_a] = 1\n",
        "            # Non-edge (inside the grid) - for any action reward 0\n",
        "            elif (r, c) not in ((0, 1), (0, 3)):\n",
        "                p[states[r_p][c_p]][rewards[0]][states[r][c]][i_a] = 1\n",
        "# A -> A' with reward +10\n",
        "p[states[4][1]][rewards[10]][states[0][1]][:] = np.ones(len(actions))\n",
        "# B -> B' with reward +5\n",
        "p[states[2][3]][rewards[5]][states[0][3]][:] = np.ones(len(actions))"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JizY96EDwuTD",
        "outputId": "0cef047a-83e2-4bd5-b6f7-7e4d7dd2d050",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Check 1: final state (0,0), reward -1, original state (0, 0). For left and up we should get probabilities 1\n",
        "p[states[0][0]][rewards[-1]][states[0][0]]"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1., 0., 1., 0.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AuxADuFB4H4E",
        "outputId": "dc879543-a2af-4843-c89f-0b296775b2ae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Check 2: For this \"up\" action should have non zero probability\n",
        "p[states[0][0]][rewards[0]][states[1][0]][:]"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1., 0., 0., 0.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fUdgaCOz1U2d"
      },
      "source": [
        "# Print matrix\n",
        "def print_v(A):\n",
        "    for r in range(side):\n",
        "        s = \"\"\n",
        "        for c in range(side):\n",
        "            s += f\" {round(A[r*side+c], 1)}\"\n",
        "        print(s)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XvTD0ro1rhEP"
      },
      "source": [
        "def iterative_policy_evaluation(gamma, epsilon):\n",
        "    # Iterative policy evaluation (p. 75)\n",
        "    # Value function\n",
        "    V = np.random.uniform(low=-1, high=1, size=states.size)\n",
        "    # V = np.zeros(states.size)\n",
        "\n",
        "    steps, max_steps = 0, 20\n",
        "    # Initial delta\n",
        "    delta = epsilon\n",
        "    # Keep max_steps in case it does not converge\n",
        "    while delta >= epsilon and steps < max_steps:\n",
        "        # Find max delta at each iteration\n",
        "        delta = 0\n",
        "        for s in range(states.size):\n",
        "            v = V[s]\n",
        "            v_new = 0\n",
        "            for a_i in range(len(actions)):\n",
        "                tmp = 0\n",
        "                for s_p in range(states.size):\n",
        "                    for r, r_i in rewards.items():\n",
        "                        tmp += p[s_p][r_i][s][a_i] * (r + gamma * V[s_p])\n",
        "                v_new = max(v_new, tmp)\n",
        "            delta = max(delta, abs(v - v_new))\n",
        "            V[s] = v_new\n",
        "        print(f\"delta: {delta}\")\n",
        "        # print_v(V)\n",
        "        steps += 1\n",
        "    print(\"Value function\")\n",
        "    print_v(V)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S8prWVo61n_C",
        "outputId": "6c572d1b-9ec5-49d8-f4b9-367a6744d2d0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "gamma = 0.5\n",
        "threshold = 0.01\n",
        "print(f\"gamma: {gamma}, epsilon: {threshold}\")\n",
        "iterative_policy_evaluation(gamma, threshold)\n",
        "\n",
        "print(\"=====================\")\n",
        "gamma = 0.9\n",
        "print(f\"gamma: {gamma}, epsilon: {threshold}\")\n",
        "iterative_policy_evaluation(gamma, threshold)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "gamma: 0.5, epsilon: 0.01\n",
            "delta: 11.322113353429108\n",
            "delta: 5.204858144981433\n",
            "delta: 2.188986666073701\n",
            "delta: 1.1628991663516537\n",
            "delta: 0.6177901821243132\n",
            "delta: 0.32820103425354574\n",
            "delta: 0.19702600992420294\n",
            "delta: 0.1231412562026275\n",
            "delta: 0.07696328512664152\n",
            "delta: 0.04810205320415051\n",
            "delta: 0.030063783252593623\n",
            "delta: 0.018789864532871903\n",
            "delta: 0.011743665333044717\n",
            "delta: 0.007339790833153614\n",
            "Value function\n",
            " 9.6 19.2 9.6 10.7 5.3\n",
            " 4.8 9.6 4.8 5.3 2.7\n",
            " 2.4 4.8 2.4 2.7 1.3\n",
            " 1.2 2.4 1.2 1.3 0.7\n",
            " 0.6 1.2 0.6 0.7 0.3\n",
            "=====================\n",
            "gamma: 0.9, epsilon: 0.01\n",
            "delta: 10.594005395254346\n",
            "delta: 14.299093878510076\n",
            "delta: 21.312656434980482\n",
            "delta: 31.76630128977407\n",
            "delta: 47.34735440939534\n",
            "delta: 70.57075827365966\n",
            "delta: 114.0838556922314\n",
            "delta: 185.84260092264498\n",
            "delta: 302.73759690298857\n",
            "delta: 493.1595453549685\n",
            "delta: 803.3568993832437\n",
            "delta: 1308.6683890953045\n",
            "delta: 2131.8208058362507\n",
            "delta: 3472.7360927072514\n",
            "delta: 5657.087095020113\n",
            "delta: 9215.394877787767\n",
            "delta: 15011.878255916276\n",
            "delta: 24454.349678887607\n",
            "delta: 39836.135626907926\n",
            "delta: 64893.064936233015\n",
            "Value function\n",
            " 34565.0 57252.1 92846.0 168055.3 151249.8\n",
            " 31108.5 51526.9 83561.4 151249.8 136124.8\n",
            " 27997.7 46374.2 75205.3 136124.8 122512.3\n",
            " 25197.9 41736.8 67684.8 122512.3 110261.1\n",
            " 22678.1 37563.1 60916.3 110261.1 99235.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gqaeW-UR4sgu"
      },
      "source": [
        "My experiments show that this example does not converge for $\\gamma > 0.7$. Probably because we can have an infinite loop - a wormhole is feeding itself - going from A to A' and back infinitevily."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3dseEN-44FeM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}